{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071aaf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'testing' from 'f:\\\\Jelena\\\\FAKULTET\\\\Student\\\\DOKTORAT\\\\projekat 2rad\\\\oop\\\\testing.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from generate_dataset import *\n",
    "from neural_net import *\n",
    "from plotting import *\n",
    "from postprocessing_dataset import *\n",
    "from predicting import *\n",
    "from preprocessing_dataset import *\n",
    "from testing import *\n",
    "\n",
    "experiment_1 = importlib.import_module(\"experiment 1\")\n",
    "importlib.reload(experiment_1)\n",
    "\n",
    "generate_dataset = importlib.import_module(\"generate_dataset\")\n",
    "importlib.reload(generate_dataset)\n",
    "\n",
    "preprocessing_datasetv = importlib.import_module(\"preprocessing_dataset\")\n",
    "importlib.reload(preprocessing_datasetv)\n",
    "\n",
    "\n",
    "postprocessing_datasetv = importlib.import_module(\"postprocessing_dataset\")\n",
    "importlib.reload(postprocessing_datasetv)\n",
    "\n",
    "imported_module = importlib.import_module(\"testing\")\n",
    "importlib.reload(imported_module)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(prec,recall):\n",
    "    prec = np.asarray(prec).mean(axis = 1)\n",
    "    rec = np.asarray(recall).mean(axis = 1)\n",
    "    f1 = np.nan_to_num(2*prec*rec/(prec+rec))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def overlaping_predictions(predictions, features):\n",
    "    overll = []\n",
    "    distance = []\n",
    "    for idx in range(0,predictions.shape[0]):\n",
    "        obj = features[idx, 1, :]\n",
    "        # overlaping = np.logical_and(predictions[idx], obj).sum()\n",
    "\n",
    "        length02  = np.where(predictions[idx] == 1)[0]\n",
    "        lengtho1 = np.where(features[idx, 1, :] == 1)[0]\n",
    "        # print(length02[0] -  lengtho1[-1])\n",
    "        # print(lengtho1[0] -  length02[-1])\n",
    "        dist = 0\n",
    "        overlaping = 0\n",
    "\n",
    "        if length02.sum() != 0:\n",
    "            dist =  max([length02[0] -  lengtho1[-1], lengtho1[0] -  length02[-1]])\n",
    "\n",
    "\n",
    "            if dist <0 :\n",
    "                overlaping = -dist\n",
    "                dist = -1\n",
    "\n",
    "\n",
    "            elif dist > 0:\n",
    "                overlaping = -1\n",
    "\n",
    "\n",
    "        distance.append(dist)\n",
    "        overll.append(overlaping)\n",
    "    return overll,distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def overlapping_type(Y_pred, X,obj_id,length_feature = 1):### \n",
    "    \"\"\"\n",
    "    za odredjeni objekat za svaki trening primer proverava koliko se sece sa objektima ciji je id manji od njegovog\n",
    "    i za svaki tr_primer vraca kolike je overlaping sa svakim od prethodnih \n",
    "    ----ostalima, dakle vraca samo jedan broj\n",
    "    \"\"\"\n",
    "    overll_per_example = []\n",
    "    distanc_per_example = []### za svaki trening primer gledam da li se preseko sa tipovima pre njega\n",
    "      \n",
    "\n",
    "    for i in range(0,X.shape[0]):#za svaki trening primer\n",
    "\n",
    "        pred= Y_pred[i]\n",
    "        j = obj_id-1# overlaping sa tipom pre njega\n",
    "        obj = X[i,length_feature+j,:]\n",
    " \n",
    "        \n",
    "        length02  = np.where(pred == 1)[0]\n",
    "        lengtho1 = np.where(obj == 1)[0]\n",
    "\n",
    "        if lengtho1.sum() == 0:\n",
    "            lengtho1 = [0]\n",
    "        dist = 0\n",
    "        overlaping = 0\n",
    "\n",
    "        if length02.sum() != 0:\n",
    "                dist =  max([length02[0] -  lengtho1[-1], lengtho1[0] -  length02[-1]])\n",
    "\n",
    "\n",
    "                if dist <0 :\n",
    "                    overlaping = -dist\n",
    "                    dist = -1\n",
    "\n",
    "\n",
    "                elif dist > 0:\n",
    "                    overlaping = -1\n",
    "\n",
    "\n",
    "        distanc_per_example.append(dist)\n",
    "        overll_per_example.append(overlaping)\n",
    "    return np.asarray(overll_per_example),np.asarray(distanc_per_example)\n",
    "\n",
    "\n",
    "\n",
    "def overlaping_predictions(predictions, features):\n",
    "    ### ne gleda overlaping z aprvi objekat posto je on svakako nula jer pre njega nema nikoga\n",
    "        testset_size = 1000\n",
    "        object_types_prediction  = list(range(0,5))\n",
    "        pred_types, X_types = split_dataset_by_type(np.asarray(predictions), testset_size),split_dataset_by_type(np.asarray(features), testset_size)\n",
    "        # pred_types, X_types = split_dataset_by_type(np.asarray(pred_pip[0]), testset_size)[1:],split_dataset_by_type(np.asarray(X_predictions[0]), testset_size)[1:]\n",
    "        print(X_types.shape)\n",
    "        print(pred_types.shape)\n",
    "\n",
    "        overlaping = []   \n",
    "        distance = []\n",
    "        for idx in object_types_prediction[1:]:\n",
    "            overlap,dist = overlapping_type( np.asarray(pred_types)[idx],np.asarray(X_types)[idx],idx,0)\n",
    "            overlaping.append(overlap)\n",
    "            distance.append(dist)\n",
    "        return  np.asarray(distance), np.asarray(overlaping)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2405316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_dataset_by_type(dataset, interval):\n",
    "    splitted = []\n",
    "    for i in range(0, int(dataset.shape[0]/interval)):\n",
    "        start = i*interval\n",
    "        end = start + interval\n",
    "        end = end if end < dataset.shape[0] else  dataset.shape[0]\n",
    "        splitted.append(dataset[start:end])\n",
    "    return np.asarray(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7eafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    import importlib\n",
    "    experiment_1 = importlib.import_module(\"experiment 1\")\n",
    "    importlib.reload(experiment_1)\n",
    "    import importlib\n",
    "    generate_dataset = importlib.import_module(\"generate_dataset\")\n",
    "    importlib.reload(generate_dataset)\n",
    "\n",
    "    predicting = importlib.import_module(\"predicting\")\n",
    "    importlib.reload(predicting)\n",
    "    \n",
    "    \n",
    "    postprocessing_datasetv = importlib.import_module(\"postprocessing_dataset\")\n",
    "    importlib.reload(postprocessing_datasetv)\n",
    "    #def experiment_per_iteration(data_train,data_test, train_set_sizes,testset_size, max_object_types) = (data_train,data_test,[100,200,500,1000,4000],1000,10)\n",
    "\n",
    "    def experiment_per_iteration(data_train,data_test, train_set_sizes,testset_size, max_object_types):\n",
    "    \n",
    "        def reset_weights(model):\n",
    "\n",
    "            import keras.backend as K\n",
    "            session = K.get_session()\n",
    "            for layer in model.layers: \n",
    "                if hasattr(layer, 'kernel_initializer'): \n",
    "                    layer.kernel.initializer.run(session=session)\n",
    "                if hasattr(layer, 'bias_initializer'):\n",
    "                    layer.bias.initializer.run(session=session)\n",
    "        max_space_length = 600\n",
    "\n",
    "        length_feature = 0\n",
    "        precissions_types_pip, recall_types_pip,precissions_types, recall_types = [],[],[],[]\n",
    "        pipeline_predictions, separate_predictions = [],[]\n",
    "\n",
    "        X_predictions = []\n",
    "\n",
    "        f1_score_pip, f1_score_seq = [],[]\n",
    "        # object_types_prediction  = list(range(0,10))\n",
    "        train_raw_dataset = experiment_1.make_train(data_train, train_set_sizes[0])\n",
    "        X_test,Y_test = experiment_1.data_preprocessing(data_test,max_object_types)\n",
    "        dist_overl_pip, dist_overlap_seq = [],[]\n",
    "\n",
    "        for train_size_id in range(len(train_set_sizes)):\n",
    "            if train_size_id > 0:        \n",
    "                train_raw_dataset = experiment_1.make_train(data_train, train_set_sizes[train_size_id])\n",
    "                reset_weights(model)\n",
    "\n",
    "            X_train,Y_train = experiment_1.data_preprocessing(train_raw_dataset,max_object_types)\n",
    "            X_test_types, Y_types =split_dataset_by_type(X_test, testset_size),split_dataset_by_type(Y_test, testset_size)\n",
    "            object_types_prediction  = list(range(0,max_object_types))\n",
    "            print(\"y_shape\",Y_types.shape)\n",
    "\n",
    "            model = run_nn((X_train, Y_train), (X_test, Y_test),5 )\n",
    "            predictions_pip_types,predictions_features = predicting.predictions_pipeline(X_test_types,model,object_types_prediction,Y_types, 0)\n",
    "\n",
    "\n",
    "            print(np.asarray(predictions_pip_types).shape)\n",
    "            predictions_pip = np.concatenate(predictions_pip_types,0)\n",
    "\n",
    "            predictions_features = np.concatenate(predictions_features)\n",
    "\n",
    "    #         precision_train_types, recall_train_types = precisions_recalls_types_ex(pred_train,Y_train,object_types_prediction)\n",
    "\n",
    "            precision_test_types, recall_test_types = precisions_recalls_types_ex(predictions_pip,Y_test,object_types_prediction)\n",
    "    #         precissions_types_train.append(precision_train_types)\n",
    "    #         recall_types_train.append(recall_train_types)\n",
    "\n",
    "            f1_score_pip.append(f1_score(precision_test_types,recall_test_types))\n",
    "            dist_overl_pip.append([overlaping_predictions(predictions_pip,predictions_features)])\n",
    "\n",
    "\n",
    "            ###sequential\n",
    "\n",
    "\n",
    "            test_conv_nn = X_test.reshape(-1, 1, max_space_length, max_object_types*2+length_feature)\n",
    "            pred = model.predict(test_conv_nn,max_object_types)\n",
    "            # pred1 = pred>0.5\n",
    "\n",
    "            print(pred.shape)\n",
    "            print(Y_test.shape)\n",
    "            pred1 = experiment_1.arrange_object(pred, Y_test)\n",
    "\n",
    "            precisions, recalls = precisions_recalls_types_ex(pred1,Y_test,object_types_prediction)\n",
    "\n",
    "            f1_score_seq.append(f1_score(precisions,recalls))\n",
    "            dist_overlap_seq.append([overlaping_predictions(pred1,X_test)])\n",
    "\n",
    "        return f1_score_pip, dist_overl_pip,f1_score_seq,dist_overlap_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27154dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### verzija van funkcije\n",
    "# (data_train,data_test, train_set_sizes,testset_size, max_object_types) = (data_train,data_test,[1000],1000,10)\n",
    "\n",
    "\n",
    "# def reset_weights(model):\n",
    "\n",
    "#     import keras.backend as K\n",
    "#     session = K.get_session()\n",
    "#     for layer in model.layers: \n",
    "#         if hasattr(layer, 'kernel_initializer'): \n",
    "#             layer.kernel.initializer.run(session=session)\n",
    "#         if hasattr(layer, 'bias_initializer'):\n",
    "#             layer.bias.initializer.run(session=session)\n",
    "# max_space_length = 600\n",
    "\n",
    "# length_feature = 0\n",
    "# precissions_types_pip, recall_types_pip,precissions_types, recall_types = [],[],[],[]\n",
    "# pipeline_predictions, separate_predictions = [],[]\n",
    "\n",
    "# X_predictions = []\n",
    "\n",
    "# f1_score_pip, f1_score_seq = [],[]\n",
    "# # object_types_prediction  = list(range(0,10))\n",
    "# train_raw_dataset = experiment_1.make_train(data_train, train_set_sizes[0])\n",
    "# X_test,Y_test = experiment_1.data_preprocessing(data_test,max_object_types)\n",
    "# dist_overl_pip, dist_overlap_seq = [],[]\n",
    "\n",
    "# for train_size_id in range(len(train_set_sizes)):\n",
    "#     if train_size_id > 0:        \n",
    "#         train_raw_dataset = experiment_1.make_train(data_train, train_set_sizes[train_size_id])\n",
    "#         reset_weights(model)\n",
    "\n",
    "#     X_train,Y_train = experiment_1.data_preprocessing(train_raw_dataset,max_object_types)\n",
    "#     X_test_types, Y_types =split_dataset_by_type(X_test, testset_size),split_dataset_by_type(Y_test, testset_size)\n",
    "#     object_types_prediction  = list(range(0,max_object_types))\n",
    "#     print(\"y_shape\",Y_types.shape)\n",
    "\n",
    "#     model = run_nn((X_train, Y_train), (X_test, Y_test),5 )\n",
    "#     predictions_pip_types,predictions_features = predicting.predictions_pipeline(X_test_types,model,object_types_prediction,Y_types, 0)\n",
    "\n",
    "\n",
    "#     print(np.asarray(predictions_pip_types).shape)\n",
    "#     predictions_pip = np.concatenate(predictions_pip_types,0)\n",
    "\n",
    "#     predictions_features = np.concatenate(predictions_features)\n",
    "\n",
    "# #         precision_train_types, recall_train_types = precisions_recalls_types_ex(pred_train,Y_train,object_types_prediction)\n",
    "\n",
    "#     precision_test_types, recall_test_types = precisions_recalls_types_ex(predictions_pip,Y_test,object_types_prediction)\n",
    "# #         precissions_types_train.append(precision_train_types)\n",
    "# #         recall_types_train.append(recall_train_types)\n",
    "\n",
    "#     f1_score_pip.append(f1_score(precision_test_types,recall_test_types))\n",
    "#     dist_overl_pip.append([overlaping_predictions(predictions_pip,predictions_features)])\n",
    "\n",
    "\n",
    "#     ###sequential\n",
    "\n",
    "\n",
    "#     test_conv_nn = X_test.reshape(-1, 1, max_space_length, max_object_types*2+length_feature)\n",
    "#     pred = model.predict(test_conv_nn,max_object_types)\n",
    "#     # pred1 = pred>0.5\n",
    "\n",
    "#     print(pred.shape)\n",
    "#     print(Y_test.shape)\n",
    "#     pred1 = experiment_1.arrange_object(pred, Y_test)\n",
    "\n",
    "#     precisions, recalls = precisions_recalls_types_ex(pred1,Y_test,object_types_prediction)\n",
    "\n",
    "#     f1_score_seq.append(f1_score(precisions,recalls))\n",
    "#     dist_overlap_seq.append([overlaping_predictions(pred1,X_test)])\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44061918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broj features\n",
      "10\n",
      "max\n",
      "5\n",
      "broj features\n",
      "10\n",
      "max\n",
      "5\n",
      "y_shape (5, 1000, 600)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1, 596, 20)        1020      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1, 596, 20)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 592, 20)        2020      \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 1, 592, 20)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 11840)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 270)               3197070   \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 270)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 600)               162600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,362,710\n",
      "Trainable params: 3,362,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Jelena\\FAKULTET\\Student\\DOKTORAT\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 6s 244ms/step - loss: 0.2804 - binary_accuracy: 0.8845 - accuracy: 0.0640 - categorical_accuracy: 0.0640 - val_loss: 0.1408 - val_binary_accuracy: 0.9359 - val_accuracy: 0.0070 - val_categorical_accuracy: 0.0070\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0999 - binary_accuracy: 0.9547 - accuracy: 0.0060 - categorical_accuracy: 0.0060 - val_loss: 0.0753 - val_binary_accuracy: 0.9661 - val_accuracy: 0.0036 - val_categorical_accuracy: 0.0036\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.0594 - binary_accuracy: 0.9738 - accuracy: 0.1200 - categorical_accuracy: 0.1200 - val_loss: 0.0605 - val_binary_accuracy: 0.9740 - val_accuracy: 0.1518 - val_categorical_accuracy: 0.1518\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.0457 - binary_accuracy: 0.9801 - accuracy: 0.1460 - categorical_accuracy: 0.1460 - val_loss: 0.0519 - val_binary_accuracy: 0.9773 - val_accuracy: 0.1332 - val_categorical_accuracy: 0.1332\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.0382 - binary_accuracy: 0.9840 - accuracy: 0.1580 - categorical_accuracy: 0.1580 - val_loss: 0.0538 - val_binary_accuracy: 0.9781 - val_accuracy: 0.1348 - val_categorical_accuracy: 0.1348\n",
      "Test loss: 0.05382484197616577\n",
      "Test accuracy: 0.9781444072723389\n",
      "dict_keys(['loss', 'binary_accuracy', 'accuracy', 'categorical_accuracy', 'val_loss', 'val_binary_accuracy', 'val_accuracy', 'val_categorical_accuracy'])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(0,1):\n",
    "    data = experiment_1.import_dataset(\"data_B.json\")\n",
    "    data_train,data_test = experiment_1.make_train_test(data, 4000, 1000)\n",
    "    results.append(experiment_per_iteration(data_train,data_test,[100, 200,500,1000,4000],1000,5))\n",
    "\n",
    "# for i in range(0,5):\n",
    "#     print(i)\n",
    "#     with open(\"exp_2_results_2pipseq\"+str(i), \"wb\") as fp:   #Pickling\n",
    "#         pickle.dump(results[i], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c747029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# # dump information to that file\n",
    "# for i in range(0,5):\n",
    "#     print(i)\n",
    "#     file = open(\"F:/Jelena/FAKULTET/Student/DOKTORAT/projekat 2rad/oop/exp_2_results_2pipseq\"+str(i), 'rb')\n",
    "#     results.append(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48b78e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m         mean_size_id\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39masarray(mean)\u001b[38;5;241m.\u001b[39mmean(axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_size_id\n\u001b[1;32m---> 11\u001b[0m f1_measure \u001b[38;5;241m=\u001b[39m mean_iteration(\u001b[43mresults\u001b[49m,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(f1_measure[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "def mean_iteration(data,measure_id):\n",
    "    mean_size_id = []\n",
    "    for train_size_id in range(5):\n",
    "        mean = []\n",
    "        for iteraions in range(5):\n",
    "            mean.append(data[iteraions][measure_id][train_size_id])\n",
    "        mean_size_id.append(np.asarray(mean).mean(axis = 0))\n",
    "        \n",
    "    return mean_size_id\n",
    "\n",
    "f1_measure = mean_iteration(results,0)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(f1_measure[i])\n",
    "plt.legend([\"100\",\"200\",\"500\",\"1000\",\"4000\"])\n",
    "np.asarray(f1_measure).mean(axis = 1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('dataaa.dat', np.around(np.asarray(f1_measure)*100, decimals=2),fmt='%.2f',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd886636",
   "metadata": {},
   "source": [
    "### Overlaping distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82584282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distover = []\n",
    "for i in range(5):\n",
    "    distover.append(results[i][1])\n",
    "distover = np.asarray(distover).squeeze()\n",
    "dist = distover[:,:,0,:,:]\n",
    "over = distover[:,:,1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a54af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ssrednji overlapihng za prmere koji overlapuju\n",
    "a = np.ma.masked_where(over<=0,over).mean(axis = 0).mean(axis =2).data\n",
    "np.savetxt('dataaa.dat', np.around(np.asarray(a), decimals=2),fmt='%.2f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ssrednji dist za prmere koji ne overlauju\n",
    "dista = np.ma.masked_where(dist<=0,dist).mean(axis = 0).mean(axis =2).data\n",
    "np.savetxt('dataaa.dat', np.around(np.asarray(dista), decimals=2),fmt='%.2f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### za exp B koliko ih bas na tacnom razmkau stavi\n",
    "#((dist%10 == 0) & (dist >0)).mean(axis = 0).mean(axis =2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f50957",
   "metadata": {},
   "outputs": [],
   "source": [
    "### koliki procenat ne overlepuje\n",
    "a = (over==0).mean(axis = 0).mean(axis = 2)\n",
    "np.savetxt('dataaa.dat', np.around(np.asarray(a)*100, decimals=2),fmt='%.2f',delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
